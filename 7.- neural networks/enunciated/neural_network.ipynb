{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# load digits dataset with 5 classes. The dataset has 10 classes in total. \n",
    "# You can change the amount of data as you like.\n",
    "num_classes = 5\n",
    "digits = load_digits(n_class=num_classes)\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "print(\"data shape: \",x.shape)\n",
    "print(\"class shape: \",y.shape)\n",
    "\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_test = x[N_train:,:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "# Add the bias term\n",
    "x_train = # Your code\n",
    "x_test =# Your code\n",
    "\n",
    "# Convert labels to one-hot vector\n",
    "y_train_onehot = # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a sigmoid activation with one-hot labels for classification, the network outputs a probability for each possible class. This is a clear advantage over using the original form of labels. For example, when the network predicts a sample as number 1 and number 3 with 50% and 40% probabilities, respectively, we know that the sample could be a number 3, but it will be more likely to be a number 1. If we don't use one-hot encoding, the output would then likely be in the range of number 2, which would be completely wrong.\n",
    "\n",
    "Check whether your one-hot conversion above is correct or not by the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_train[:5])\n",
    "print(y_train_onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Forward computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most deep learning frameworks provide a list of basic functions as building blocks, such as fully_connected, sigmoid, relu... so that you can stack them sequentially as layers to build your own neural networks. In this exercise, we will see implement the fully connected layer and the sigmoid activation function. In each function, we will return the result and the cache the input for backward computation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function returns the probability of y=1\n",
    "def sigmoid(x):\n",
    "    cache = x\n",
    "    result = 1.0/(1 + np.exp(-x))\n",
    "    return cache, result\n",
    "\n",
    "def fully_connected(x, theta):\n",
    "    cache = (x, theta)\n",
    "    result = # Your code\n",
    "    return cache, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having our building blocks, we can start stacking layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_forward(x, theta_matrices):\n",
    "    '''\n",
    "    x: feature vector\n",
    "    theta_matrices: The list contains all theta. The first element is the theta (matrix) of the input layer and the first hidden\n",
    "    layer, the second one is the theta of the fist hidden layer and the second hidden layer, and so on\n",
    "    \n",
    "    In this exercise, our network architecture will be: \n",
    "    input -> fully_connected -> sigmoid -> fully_connected -> sigmoid -> output\n",
    "    You don't need to use regularization in this exercise\n",
    "    '''\n",
    "    result = x\n",
    "    cache = dict() \n",
    "    for i, theta in enumerate(theta_matrices) :\n",
    "        ## Your code here, should be a result of a fully_connected layer then a sigmoid activation.\n",
    "        # Store the result of each computation in cache, for doing backprop later.\n",
    "        # For this exercise, cache should have four items with keys: fc0, sigmoid0, fc1, sigmoid1\n",
    "    return cache, result\n",
    "\n",
    "def compute_cost(outputs, labels):\n",
    "    '''mean square error'''\n",
    "    result = ## Your code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize theta and check the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden = 100\n",
    "theta0 = np.random.normal(loc=0., scale=0.5, size=(n_features+1, num_hidden+1)) # + 1 for bias term\n",
    "theta1 = np.random.normal(loc=0., scale=0.5, size=(num_hidden+1, num_classes))\n",
    "theta_matrices = [theta0, theta1]\n",
    "cache, initial_outputs = compute_forward(x_train, theta_matrices)\n",
    "assert initial_outputs.shape == y_train_onehot.shape, 'forward pass returns wrong shape'\n",
    "print('forward pass returns correct shape')\n",
    "initial_cost = compute_cost(initial_outputs, y_train_onehot)\n",
    "print(initial_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to forward pass, calculating backward gradient using backpropagation is just like stacking several layers of gradient together. To do so, we first need to calculate the gradient of each of our building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(cache, result):\n",
    "    x = cache\n",
    "    sigmoid_grad = sigmoid(x)[1] * (1 - sigmoid(x)[1])\n",
    "    return sigmoid_grad * result\n",
    "    \n",
    "def fc_backward(cache, result):\n",
    "    x, theta = cache\n",
    "    theta_grad = x.T.dot(result)\n",
    "    x_grad = result.dot(theta.T)\n",
    "    return x_grad, theta_grad\n",
    "\n",
    "def cost_backward(outputs, labels):\n",
    "    ## your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_backprop(x, theta_matrices, cache, outputs, labels):\n",
    "    '''\n",
    "    return gradients for theta_matrices\n",
    "    '''\n",
    "    theta_grad = {} # should include two key theta0 and theta1 for this exercise\n",
    "    grad = cost_backward(outputs, labels)\n",
    "    for i, theta in enumerate(theta_matrices[::-1]):\n",
    "        layer = len(theta_matrices) - i - 1 # first iteration: layer 1, second iter: layer 0\n",
    "        # Your code, first you need to propagate the gradient through the sigmoid activation,\n",
    "        # then through the fully_connected layer\n",
    "    return theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if compute_backprop returns the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_grad = compute_backprop(x_train, theta_matrices, cache, initial_outputs, y_train_onehot)\n",
    "dtheta0 = theta_grad['theta0']\n",
    "assert dtheta0.shape == theta0.shape, 'backprop returns wrong shape for theta 0'\n",
    "dtheta1 = theta_grad['theta1']\n",
    "assert dtheta1.shape == theta1.shape, 'backprop returns wrong shape for theta 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have both forward and backward computation, use batch gradient descent to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.001 # learning rate\n",
    "N_iterations = 200 # You can play with this one to see how the network performs\n",
    "J = np.zeros(N_iterations)\n",
    "\n",
    "for i in range(N_iterations):\n",
    "    ## Your code\n",
    "    # First, do a forward pass\n",
    "    ...\n",
    "    # Calculate the cost and store it in J[i]\n",
    "    ...\n",
    "    # Calculate the gradients by doing a backward pass\n",
    "    ...\n",
    "    # Update the weights by gradient descent rule\n",
    "    ....\n",
    "\n",
    "# calculate the loss on the whole training set \n",
    "J_train = compute_cost(compute_forward(x_train, theta_matrices)[1], y_train_onehot)\n",
    "print('training cost: %f' %J_train)\n",
    "\n",
    "# plot cost function\n",
    "plt.plot(J)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your accuracy on test set should be greater than 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_ground_truth,y_pred):\n",
    "    ### YOUR CODE GOES  HERE ###\n",
    "\n",
    "pred_one_max = compute_forward(x_test, theta_matrices)[1]\n",
    "\n",
    "# Your prediction would be an one-hot vector, for each test sample, select the one with the highest probablity to assign the class\n",
    "pred = ## Your code\n",
    "accuracy = compute_accuracy(y_test, pred)\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Can you do better than the above model?\n",
    "You can try to tune hyper-parameters, adding regularization, stacking more layers or implement different activation functions (tanh, ReLU)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
